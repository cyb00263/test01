{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_tictactoe_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEMQnAu0kfVMq8pEntufW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyb00263/test01/blob/main/dqn_tictactoe_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpGmWbFCkYt0",
        "outputId": "2be9106a-29bc-4ac4-f824-dacd73787f85"
      },
      "source": [
        "#installing dependencies\r\n",
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\r\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\r\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\r\n",
        "\r\n",
        "!pip -q install gym\r\n",
        "!pip -q install pyglet\r\n",
        "!pip -q install pyopengl\r\n",
        "!pip -q install pyvirtualdisplay\r\n",
        "\r\n",
        "# Start virtual display\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1024, 768))\r\n",
        "display.start()\r\n",
        "import os\r\n",
        "#os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Unable to locate package libcusparse8.0\n",
            "E: Couldn't find any package by glob 'libcusparse8.0'\n",
            "E: Couldn't find any package by regex 'libcusparse8.0'\n",
            "E: Unable to locate package libnvrtc8.0\n",
            "E: Couldn't find any package by glob 'libnvrtc8.0'\n",
            "E: Couldn't find any package by regex 'libnvrtc8.0'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eFyap0zkYcN",
        "outputId": "4e742aa2-632c-43d4-82df-069ececc89df"
      },
      "source": [
        "!pip install tensorflow==1.14\r\n",
        "!pip install keras==2.2.4\r\n",
        "!pip install keras-rl==0.4.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (51.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.19.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: keras-rl==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl==0.4.2) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.19.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr4Ch4cHsTOp"
      },
      "source": [
        "import math\r\n",
        "import gym\r\n",
        "from gym import spaces, logger\r\n",
        "from gym.utils import seeding\r\n",
        "import numpy as np\r\n",
        "import copy\r\n",
        "import random\r\n",
        "\r\n",
        "class Practice(gym.core.Env):\r\n",
        "    def __init__(self):\r\n",
        "        self.n_action = 5*5\r\n",
        "        self.board = [0] * self.n_action\r\n",
        "        self.action_space = gym.spaces.Discrete(self.n_action) # actionの取りうる値\r\n",
        "        self.observation_space = gym.spaces.Box(low=-1, high =1, shape=(self.n_action,)) \r\n",
        "        self.result = []\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "      self.board, end_flg, reward = self.get_input(self.board, action)\r\n",
        "      done = end_flg\r\n",
        "      info = {}\r\n",
        "\r\n",
        "      return self.board, reward, done, info\r\n",
        "\r\n",
        "    def get_input(self, board, action):\r\n",
        "      end_flg = 0\r\n",
        "      reward = 0\r\n",
        "      #AI#\r\n",
        "      space = [i for i, x in enumerate(board) if x == 0 ]\r\n",
        "      if action in space:\r\n",
        "        flg = 1\r\n",
        "        board[action] = flg\r\n",
        "        end_flg, reward = self.judge(board)\r\n",
        "        if end_flg == 0:\r\n",
        "          space = [i for i, x in enumerate(board) if x == 0 ]\r\n",
        "          ransu = int(random.choice(space))\r\n",
        "          flg = -1\r\n",
        "          board[ransu] = flg\r\n",
        "          end_flg, reward = self.judge(board)\r\n",
        "          if end_flg == 1:\r\n",
        "            reward = -5\r\n",
        "            self.result.append(reward)\r\n",
        "      else:\r\n",
        "        end_flg = 1\r\n",
        "        reward = -5\r\n",
        "        self.result.append(reward)\r\n",
        "\r\n",
        "      return board, end_flg, reward\r\n",
        "\r\n",
        "    def judge(self, board):\r\n",
        "      end_flg = 0\r\n",
        "      reward = 0\r\n",
        "      mass = 5\r\n",
        "      moku = 5\r\n",
        "      score = []\r\n",
        "\r\n",
        "      space = [i for i, x in enumerate(board) if x == 0 ]\r\n",
        "      if len(space) == 0:\r\n",
        "        end_flg = 1\r\n",
        "        reward = 0\r\n",
        "      else:\r\n",
        "        board = np.array(board).reshape(5,5)\r\n",
        "        for i in range(mass-moku+1):\r\n",
        "          for j in range(mass-moku+1):\r\n",
        "            for k in range(moku):\r\n",
        "              score_r = board[i][j+k] + board[i+1][j+k] + board[i+2][j+k] + board[i+3][j+k] + board[i+4][j+k]\r\n",
        "              score.append(score_r)\r\n",
        "              score_c = board[i+k][j] + board[i+k][j+1] + board[i+k][j+2] + board[i+k][j+3] + board[i+k][j+4]\r\n",
        "              score.append(score_c)\r\n",
        "            score_dl = board[i][j] + board[i+1][j+1] + board[i+2][j+2] + board[i+3][j+3] + board[i+4][j+4]\r\n",
        "            score.append(score_dl)\r\n",
        "            score_dr = board[i][j+4] + board[i+1][j+3] + board[i+2][j+2] + board[i+3][j+1] + board[i+4][j]\r\n",
        "            score.append(score_dr)\r\n",
        "        if max(score) == 5:\r\n",
        "          end_flg = 1\r\n",
        "          reward = 25\r\n",
        "        elif min(score) == -5:\r\n",
        "          end_flg = 1\r\n",
        "          reward = -5\r\n",
        "        else:\r\n",
        "          reward = min(score)/5\r\n",
        "\r\n",
        "      return end_flg, reward\r\n",
        "\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.board = [0] * self.n_action\r\n",
        "        return self.board\r\n",
        "\r\n",
        "    def render(self, mode):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def close(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def seed(self):\r\n",
        "        pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD_BrfZckXPv",
        "outputId": "65e4aa98-541a-4bf0-df3f-bafb30bafd0b"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Activation, Flatten\r\n",
        "from keras.optimizers import Adam\r\n",
        "from rl.agents.dqn import DQNAgent\r\n",
        "from rl.policy import BoltzmannQPolicy\r\n",
        "from rl.memory import SequentialMemory\r\n",
        "\r\n",
        "practice = Practice()\r\n",
        "n_action = 5*5\r\n",
        "\r\n",
        "# ニューラルネットワークの構造を定義\r\n",
        "model = Sequential()\r\n",
        "model.add(Flatten(input_shape=(1,) + practice.observation_space.shape))\r\n",
        "model.add(Dense(128))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(Dense(n_action))\r\n",
        "model.add(Activation('linear'))\r\n",
        "print(model.summary()) # モデルの定義をコンソールに出力\r\n",
        " \r\n",
        "# モデルのコンパイル\r\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\r\n",
        "policy = BoltzmannQPolicy(tau=1.)\r\n",
        "dqn = DQNAgent(model=model, nb_actions=n_action, memory=memory, nb_steps_warmup=50, target_model_update=1e-2, policy=policy)\r\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               3328      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 25)                3225      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 25)                0         \n",
            "=================================================================\n",
            "Total params: 6,553\n",
            "Trainable params: 6,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TX9nrSUb65Rk",
        "outputId": "56d8d39b-2840-4c61-a87b-2ddc718d39a7"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# fit の結果を取得しておく\r\n",
        "history = dqn.fit(practice, nb_steps=500000, visualize=False, verbose=1)\r\n",
        "\r\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(\"TicTacToe\"), overwrite=True)\r\n",
        "\r\n",
        "# 結果を表示\r\n",
        "plt.subplot(2,1,1)\r\n",
        "plt.plot(history.history[\"nb_episode_steps\"])\r\n",
        "plt.ylabel(\"step\")\r\n",
        "\r\n",
        "plt.subplot(2,1,2)\r\n",
        "plt.plot(history.history[\"episode_reward\"])\r\n",
        "plt.xlabel(\"episode\")\r\n",
        "plt.ylabel(\"reward\")\r\n",
        "\r\n",
        "plt.show()  # windowが表示されます。"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 500000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: -1.1234\n",
            "1755 episodes - episode_reward: -6.399 [-10.600, 24.000] - loss: 1.084 - mean_absolute_error: 2.337 - mean_q: 0.247\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 49s 5ms/step - reward: 0.0191\n",
            "1326 episodes - episode_reward: 0.142 [-12.600, 24.200] - loss: 1.799 - mean_absolute_error: 3.632 - mean_q: 4.314\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 48s 5ms/step - reward: 1.3726\n",
            "1188 episodes - episode_reward: 11.554 [-12.600, 24.200] - loss: 3.089 - mean_absolute_error: 10.467 - mean_q: 15.631\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 1.7969\n",
            "1244 episodes - episode_reward: 14.444 [-12.600, 24.400] - loss: 3.118 - mean_absolute_error: 12.287 - mean_q: 17.522\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 52s 5ms/step - reward: 1.8902\n",
            "1261 episodes - episode_reward: 14.991 [-12.600, 24.400] - loss: 3.433 - mean_absolute_error: 12.507 - mean_q: 17.598\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 1.9981\n",
            "1281 episodes - episode_reward: 15.597 [-12.400, 24.400] - loss: 3.829 - mean_absolute_error: 12.382 - mean_q: 17.436\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 49s 5ms/step - reward: 2.0969\n",
            "1287 episodes - episode_reward: 16.296 [-12.800, 24.400] - loss: 3.838 - mean_absolute_error: 12.543 - mean_q: 17.888\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 49s 5ms/step - reward: 2.0694\n",
            "1252 episodes - episode_reward: 16.527 [-12.600, 24.200] - loss: 3.504 - mean_absolute_error: 13.182 - mean_q: 19.060\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 49s 5ms/step - reward: 2.2037\n",
            "1277 episodes - episode_reward: 17.256 [-13.200, 24.200] - loss: 3.352 - mean_absolute_error: 13.854 - mean_q: 20.122\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 2.4458\n",
            "1324 episodes - episode_reward: 18.473 [-12.800, 24.200] - loss: 3.297 - mean_absolute_error: 14.252 - mean_q: 20.872\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 2.4470\n",
            "1322 episodes - episode_reward: 18.509 [-12.600, 24.400] - loss: 3.118 - mean_absolute_error: 14.601 - mean_q: 21.440\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5584\n",
            "1337 episodes - episode_reward: 19.136 [-12.800, 24.200] - loss: 2.982 - mean_absolute_error: 14.875 - mean_q: 21.841\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.5951\n",
            "1332 episodes - episode_reward: 19.483 [-12.000, 24.400] - loss: 2.723 - mean_absolute_error: 15.158 - mean_q: 22.238\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 2.6967\n",
            "1351 episodes - episode_reward: 19.961 [-12.000, 24.200] - loss: 2.471 - mean_absolute_error: 15.387 - mean_q: 22.639\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.5808\n",
            "1321 episodes - episode_reward: 19.537 [-12.200, 24.200] - loss: 2.335 - mean_absolute_error: 15.583 - mean_q: 22.910\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6315\n",
            "1325 episodes - episode_reward: 19.861 [-12.200, 24.200] - loss: 2.293 - mean_absolute_error: 15.663 - mean_q: 23.045\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5058\n",
            "1312 episodes - episode_reward: 19.098 [-12.200, 24.400] - loss: 2.214 - mean_absolute_error: 15.634 - mean_q: 23.081\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 52s 5ms/step - reward: 2.6727\n",
            "1339 episodes - episode_reward: 19.961 [-12.200, 24.200] - loss: 2.297 - mean_absolute_error: 15.603 - mean_q: 23.107\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 2.6852\n",
            "1331 episodes - episode_reward: 20.174 [-12.400, 24.200] - loss: 2.200 - mean_absolute_error: 15.680 - mean_q: 23.194\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.7145\n",
            "1341 episodes - episode_reward: 20.243 [-12.600, 24.200] - loss: 2.145 - mean_absolute_error: 15.755 - mean_q: 23.350\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 2.6544\n",
            "1332 episodes - episode_reward: 19.927 [-12.800, 24.200] - loss: 2.167 - mean_absolute_error: 15.739 - mean_q: 23.305\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6638\n",
            "1335 episodes - episode_reward: 19.956 [-13.200, 24.200] - loss: 2.127 - mean_absolute_error: 15.859 - mean_q: 23.464\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6174\n",
            "1327 episodes - episode_reward: 19.722 [-12.400, 24.200] - loss: 2.037 - mean_absolute_error: 15.923 - mean_q: 23.490\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6918\n",
            "1337 episodes - episode_reward: 20.133 [-12.200, 24.400] - loss: 2.135 - mean_absolute_error: 15.911 - mean_q: 23.384\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 50s 5ms/step - reward: 2.8479\n",
            "1363 episodes - episode_reward: 20.895 [-12.600, 24.200] - loss: 2.114 - mean_absolute_error: 16.009 - mean_q: 23.519\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.7888\n",
            "1347 episodes - episode_reward: 20.704 [-11.800, 24.400] - loss: 2.004 - mean_absolute_error: 16.086 - mean_q: 23.575\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6416\n",
            "1331 episodes - episode_reward: 19.846 [-12.600, 24.200] - loss: 1.963 - mean_absolute_error: 16.150 - mean_q: 23.652\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6693\n",
            "1325 episodes - episode_reward: 20.145 [-12.600, 24.200] - loss: 1.957 - mean_absolute_error: 16.176 - mean_q: 23.685\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.5750\n",
            "1308 episodes - episode_reward: 19.689 [-12.800, 24.200] - loss: 1.929 - mean_absolute_error: 16.209 - mean_q: 23.699\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 54s 5ms/step - reward: 2.7073\n",
            "1337 episodes - episode_reward: 20.247 [-12.800, 24.200] - loss: 2.062 - mean_absolute_error: 16.164 - mean_q: 23.573\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6205\n",
            "1309 episodes - episode_reward: 20.019 [-12.000, 24.200] - loss: 2.097 - mean_absolute_error: 16.124 - mean_q: 23.494\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.7064\n",
            "1342 episodes - episode_reward: 20.167 [-12.600, 24.200] - loss: 2.113 - mean_absolute_error: 16.158 - mean_q: 23.543\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.7352\n",
            "1329 episodes - episode_reward: 20.582 [-12.000, 24.200] - loss: 2.064 - mean_absolute_error: 16.205 - mean_q: 23.601\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6849\n",
            "1327 episodes - episode_reward: 20.231 [-12.400, 24.200] - loss: 1.980 - mean_absolute_error: 16.206 - mean_q: 23.634\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6552\n",
            "1315 episodes - episode_reward: 20.192 [-12.200, 24.200] - loss: 1.913 - mean_absolute_error: 16.215 - mean_q: 23.587\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5969\n",
            "1319 episodes - episode_reward: 19.688 [-12.400, 24.200] - loss: 1.963 - mean_absolute_error: 16.296 - mean_q: 23.592\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 54s 5ms/step - reward: 2.6828\n",
            "1332 episodes - episode_reward: 20.141 [-12.000, 24.200] - loss: 1.992 - mean_absolute_error: 16.300 - mean_q: 23.592\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 52s 5ms/step - reward: 2.7453\n",
            "1338 episodes - episode_reward: 20.520 [-11.600, 24.200] - loss: 1.966 - mean_absolute_error: 16.311 - mean_q: 23.656\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.7533\n",
            "1341 episodes - episode_reward: 20.529 [-12.000, 24.200] - loss: 1.944 - mean_absolute_error: 16.252 - mean_q: 23.575\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6415\n",
            "1311 episodes - episode_reward: 20.149 [-12.200, 24.200] - loss: 1.882 - mean_absolute_error: 16.291 - mean_q: 23.669\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.4997\n",
            "1288 episodes - episode_reward: 19.409 [-12.800, 24.200] - loss: 1.903 - mean_absolute_error: 16.282 - mean_q: 23.631\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6846\n",
            "1321 episodes - episode_reward: 20.322 [-12.400, 24.400] - loss: 1.990 - mean_absolute_error: 16.343 - mean_q: 23.709\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 53s 5ms/step - reward: 2.6231\n",
            "1317 episodes - episode_reward: 19.916 [-12.600, 24.200] - loss: 2.082 - mean_absolute_error: 16.296 - mean_q: 23.600\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.7185\n",
            "1341 episodes - episode_reward: 20.273 [-12.800, 24.200] - loss: 2.223 - mean_absolute_error: 16.277 - mean_q: 23.581\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6534\n",
            "1323 episodes - episode_reward: 20.055 [-12.400, 24.200] - loss: 2.115 - mean_absolute_error: 16.299 - mean_q: 23.608\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.7028\n",
            "1332 episodes - episode_reward: 20.293 [-12.000, 24.400] - loss: 2.049 - mean_absolute_error: 16.312 - mean_q: 23.643\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6023\n",
            "1303 episodes - episode_reward: 19.970 [-12.800, 24.200] - loss: 2.050 - mean_absolute_error: 16.306 - mean_q: 23.616\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.5729\n",
            "1293 episodes - episode_reward: 19.899 [-12.400, 24.200] - loss: 1.965 - mean_absolute_error: 16.328 - mean_q: 23.613\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 52s 5ms/step - reward: 2.5688\n",
            "1301 episodes - episode_reward: 19.746 [-12.000, 24.200] - loss: 2.006 - mean_absolute_error: 16.290 - mean_q: 23.557\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 51s 5ms/step - reward: 2.6407\n",
            "done, took 2569.680 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU9ZnH8c/DDPd9OwrMgKgcKgrjgRJFRcVjPeIRTTxi4npuYmLWAKsxiXGzxrjRmEtdV2MS72hiAvFAQuLFioOAgHKJoKAcKijnMMezf9Rvhu6ZAQbomu6e+r5fr6arflVd9e2Z4umaX1VXmbsjIiLJ0SLbAUREpGmp8IuIJIwKv4hIwqjwi4gkjAq/iEjCFGY7QGP06NHDS0pKsh1DRCSvzJgx42N371m3PS8Kf0lJCWVlZdmOISKSV8xsWUPt6uoREUkYFX4RkYTJi66euFVUVbPfjc9mO4aISJojB3TjsStGZny52uMH1m7amu0IIiL1/N+ST2NZrgq/iEjCqPCLiCSMCj9gWLYjiIg0GRV+wFT3RSRBVPhB+/sikigq/CIiCZO4wv/snI944o0PasfveH4BI259MYuJRES2r7KqOuPLTFzhv/rhN/nuU2/Vjv9y6uIsphER2bFnZn2Y8WUmrvCLiCSdCr+ISA6L46xDFX4RkRymwi8ikjBxfMFUhV9EJIdpj19ERPZYsy787s5Dry1lQ3llvWkl4ydx/8tLspBKRCS7mnXhf2Xxx3z/L/P44V/mAbB2Y/p192+d9E42YomINJrF0NfTrAv/5q1VAKzdVAFAZbVnM46IyC6L41pizbrwb+PhXxV+EckvOri7i+L4E0lEpCnpdE4RkYTJqz1+M3vAzFab2dyUtm5mNtnMFoXnrnGtP5Wrh0dEpFace/y/BcbWaRsPTHH3/YApYVxERLajRQx7/IWZX2TE3V8ys5I6zWcCo8PwQ8A/gHFxZagxZf5q7py8kElzPop7VSIiGZb5yh9b4d+O3u5eU31XAr23N6OZXQFcAdCvX789XvHPpyza42WIiDQHWTu46+4O2z+/0t3vc/dSdy/t2bPnbq1D5/SISL7Lq4O727HKzIoAwvPqJl6/iEheaQ5f4PoLcGkYvhR4ponXLyKSV/Lqkg1m9igwDTjAzJab2deB24ATzWwRMCaMi4jIdsSxxx/nWT0XbmfSCXGtU0REdq5Zf3NXV2wQkXzXHA7uNql14aqcIiL5qryyOuPLbNaF/ztPzs52BBGRPfLhus0ZX2azLvwiIlKfCr+ISMKo8IuIJIwKv4hIwjSq8JvZADP7q5l9HK6x/4yZDYg7nIiIZF5j9/gfAZ4A9gL2Bp4EHo0rlIiIRLJ5yYZ27v57d68Mjz8AbTKeRkRE0mTzkg3Pmtl44DGiSyl/CfibmXUDcPdPY8gmIiIxaGzhPz88X1mn/QKiDwL194uI5IlGFX537x93EBERqS9r1+oxs3ZmdpOZ3RfG9zOz0zMfR0RE4tbYg7sPAluBo8L4CuDWWBKJiEisGlv493X324EKAHffhG5pKyISu2zeenGrmbUl3BzdzPYFymPIIyIiKeI4j7+xZ/X8AHgO6GtmDwNHA5dlPI2IiMSusWf1vGBmM4Ajif7yuM7dP441mYiI4O4ZX2Zjz+qZ4u6fuPskd5/o7h+b2ZSMpxERkTQFLZq4q8fM2gDtgB5m1pVtxxk6AftkPI2IiKTLQh//lcC3iC7MNoOo8DuwHvhFxtOIiEiaJj+rx91/Hr61+5/AIWH4QWAJMC2GPCIikiJr39wFznX3z81sFHA8cD/wm8zHERGRuDW28FeF59OA/3H3SUCreCKJiEicGlv4V5jZvWy7HHPrXXitiIjsJouhl7+xxft84HngZHdfB3QDbsh4GhERiV1jv8C1CXg6Zfwj4KO4QomISHzUXSMiksOyeVaPiIhkQTavzikiIlmgPX4REdljjb0sc0aZ2VKiyz5UAZXuXpqNHCIiSZSVwh8cp0s7i4jsWDbP4xcRkWYiW4XfgRfMbIaZXdHQDGZ2hZmVmVnZmjVrmjieiEhuGH1Az4wvM1tdPaPcfYWZ9QImm9l8d38pdQZ3vw+4D6C0tDTzt6DZgaW3ndaUqxPJa6N/OpWln2xi6r+Ppn+P9tmOs1M/eW4+v/nHu9xw8gFce9xANpRXcuD3n6d9qwLm3TIWgJLxk4DmWwuyssfv7ivC82rgT8Dh2cghIpkTxy0CJR5NXvjNrL2ZdawZBk4C5jZ1DhHJDIvjRPMY6fMpO109vYE/hY2lEHjE3Z/LQg4RyYB829N3orx59nmVUU1e+N19CTCsqdcrIvHKmz3/8DkVx2mS+UKnc4pIotT8fZIvn1NxUOEXkUSp6ZpKcN1X4ReRPXP+YX0B6N4hP+7GWnNIYkd7/JeMLG6aMFmSzUs2iEgzcPWx+3LlMftS0CI/9qFru3p2sM//wzOG8v1/Gdo0gbJAhV9E9oiZUZAfNR9o3B5/vr2nXaWuHhGRhFHhF5FEcfLrewdxUOEXkUTZ1tXTjPtydkKFX0QSKbllX4VfRBKm9jz+BFd+Ff46jhzQLdsRRCRGQ/fpDMDAXh0AaBlO3zk2huve5yqdzpni118ZznEH9Mp2DBGJ0Xkj+jC8XxcG9uoIQOvCAl664Th6dWqd5WRNR4U/xakHFWU7gojEzMxqi36Nft3bZSlNdqirR0QkYVT4RUQSRoVfRCRhVPhFRBJGhV9EJGFU+EVEEqZZF/4Xrz+mwfbUy4bfcd4wThzSm0f+9YgmSiUikl1W8/XlXFZaWuplZWXZjiEiklfMbIa7l9Ztb9Z7/CIiUp8Kv4hIwuRFV4+ZrQGW7ebLewAfZzBOU1HupqXcTUu5m0axu9e7+lxeFP49YWZlDfVx5TrlblrK3bSUO7vU1SMikjAq/CIiCZOEwn9ftgPsJuVuWsrdtJQ7i5p9H7+IiKRLwh6/iIikUOEXEUmYZl34zWysmS0ws8VmNj4L63/AzFab2dyUtm5mNtnMFoXnrqHdzOzukPUtMxue8ppLw/yLzOzSlPYRZjYnvOZuMzMywMz6mtlUM3vbzOaZ2XX5kN3M2pjZdDObHXL/MLT3N7PXw7oeN7NWob11GF8cppekLGtCaF9gZientMe2TZlZgZnNNLOJ+ZLbzJaG3+MsMysLbTm9nYTldjGzP5rZfDN7x8xG5kPujHH3ZvkACoB3gQFAK2A2MKSJMxwDDAfmprTdDowPw+OBn4ThU4FnAQOOBF4P7d2AJeG5axjuGqZND/NaeO0pGcpdBAwPwx2BhcCQXM8eltUhDLcEXg/reAK4ILTfA1wdhq8B7gnDFwCPh+EhYXtpDfQP21FB3NsUcD3wCDAxjOd8bmAp0KNOW05vJ2G5DwGXh+FWQJd8yJ2xbS3bAWJ7YzASeD5lfAIwIQs5Skgv/AuAojBcBCwIw/cCF9adD7gQuDel/d7QVgTMT2lPmy/D7+EZ4MR8yg60A94EjiD6pmVh3e0CeB4YGYYLw3xWd1upmS/ObQroA0wBjgcmhhz5kHsp9Qt/Tm8nQGfgPcLJLfmSO5OP5tzVsw/wQcr48tCWbb3d/aMwvBLoHYa3l3dH7csbaM+o0I1wKNHec85nD90ls4DVwGSiPd117l7ZwLpq84XpnwHdd+P9ZMJdwHeB6jDePU9yO/CCmc0wsytCW65vJ/2BNcCDoWvtfjNrnwe5M6Y5F/6c59HuQM6eT2tmHYCngG+5++ep03I1u7tXufshRHvQhwODshxpp8zsdGC1u8/IdpbdMMrdhwOnANeaWdpNMHJ0Oykk6oL9jbsfCmwk6tqplaO5M6Y5F/4VQN+U8T6hLdtWmVkRQHheHdq3l3dH7X0aaM8IM2tJVPQfdven8yk7gLuvA6YSdXN0MbPCBtZVmy9M7wx8spPccWxTRwNnmNlS4DGi7p6f50Fu3H1FeF4N/InowzbXt5PlwHJ3fz2M/5HogyDXc2dOtvua4noQfaovIfqzruaA1tAs5CghvY//p6QfQLo9DJ9G+gGk6aG9G1F/ZNfweA/oFqbVPYB0aoYyG/A74K467TmdHegJdAnDbYGXgdOBJ0k/SHpNGL6W9IOkT4ThoaQfJF1CdIA09m0KGM22g7s5nRtoD3RMGX4NGJvr20lY7svAAWH4ByFzzufO2HaW7QCxvrnoaPxCon7eG7Ow/keBj4AKor2MrxP1xU4BFgEvpmwoBvwqZJ0DlKYs52vA4vC4LKW9FJgbXvNL6hys2oPco4j+zH0LmBUep+Z6duBgYGbIPRe4ObQPCP8RFxMV09ahvU0YXxymD0hZ1o0h2wJSzsiIe5sivfDndO6Qb3Z4zKtZbq5vJ2G5hwBlYVv5M1HhzvncmXrokg0iIgnTnPv4RUSkASr8IiIJo8IvIpIwhTufJft69OjhJSUl2Y4hIpJXZsyY8bE3cM/dvCj8JSUllJWVZTuGiEheMbNlDbWrq0dEJGFU+PeAu7Nw1fra8aUfb2RLRRXvrtlARVU1qz/fwvT3PmV7p8wuXLV+u9MyadGq9VRX73g97328kfLKqoyvu+7PKN+sXr+FtRu3NmrexavXU7WTn7PEZ/Xnjf9d7YnPNlWw8rMtsa9nzfpyPtlQHsuy86KrJy6LVq2nqEtbOrQuZNYH6zjvnteoqNJ/XBHJHe/++FQKWmT2cv6J2uPftLWSme+vrR0/8c6XuPSB6by0cA1n/epVFX0RyTkrP8/8XxeJKvzXPTaLs3/9Gp+m/Dk4Y9la3vt4YxZTiYhs3zsffr7zmXZRorp65iz/DID7XlrCk2XbLqP9/b/My1YkEZEdWhNDP38iCv+b76+lVcG2P27u+ee7WUwjItJ4cZwwkIjC/8VfvwbAXp3aZDmJiMiuiaPwJ6qP35vvDXVEpJmqVOEXEUmWqurqnc+0i2Ir/GbW18ymmtnbZjbPzK4L7d3MbLKZLQrPXePKICKS7w7Yq1PGlxnnHn8l8B13H0J0C7JrzWwI0S3Nprj7fkR3uxm/g2WIiCTaiOLM7xvHVvjd/SN3fzMMrwfeAfYBzgQeCrM9BJwVV4b6mZpqTSIimZHZ7+xGmqSP38xKgEOB14He7v5RmLQS6L2d11xhZmVmVrZmzZqmiCkikgixF34z6wA8BXzL3dO+gubRFcoa3A939/vcvdTdS3v2rHc5aRER2U2xFn4za0lU9B9296dD8yozKwrTi4DVcWYQEZF0cZ7VY8D/Au+4+89SJv0FuDQMXwo8E1eGutTFLyL5xmLo5I/zm7tHAxcDc8xsVmj7D+A24Akz+zqwDDg/xgwiIlJHbIXf3V9h+wekT4hrvSIismP65q6ISMIkqvDrPH4RyTcWw5n8iSr8IiKiwi8ikjgJK/zq6xERSVThVx+/iOSbOM7jT1ThFxERFX4RkcRR4RcRSZhEFX518YuIJKzwi4iICr+ISOKo8IuIJEyiCr/rRH4RyTM6j19ERPbYDq/Hb2Z/ZQcnw7j7GRlPJCIisdrZjVjuCM9fBPYC/hDGLwRWxRVKRETis8PC7+7/BDCz/3b30pRJfzWzsliTxUA9/CKSb7J5Pf72ZjagNohZf6B9xtOIiEjsGnvP3W8B/zCzJUT30S0GrogtlYiIxGanhd/MWgCdgf2AQaF5vruXxxlMRETisdOuHnevBr7r7uXuPjs88rLo6zR+Eck32TyP/0Uz+3cz62tm3WoemY8TL32BS0Sk8X38XwrP16a0OTCggXlFRCSHNarwu3v/uIOIiEjTaOweP2Z2IDAEaFPT5u6/iyNUXNTRIyL5JoYu/sYVfjP7PjCaqPD/DTgFeAXIq8IvIiKNP7h7LnACsNLdLwOGEZ3iKSIieaaxhX9zOK2z0sw6AauBvvHFEhGRuDS2j7/MzLoA/wPMADYA02JLJSIiAFgMJ/I39qyea8LgPWb2HNDJ3d/KeJq46eiuiEijD+7+HngJeNnd58cbSURE4tTYPv4HgCLgF2a2xMyeMrPrYswlIiIxaWxXz1Qzewk4DDgOuAoYCvw8xmwiIomXzfP4pxBdf38a8DJwmLuvjiFPrNTFLyLS+K6et4CtwIHAwcCBZtY2tlQiIhKbxnb1fBvAzDoCXwUeJLoHb+vYkomISCwa29Xzb8AXgBHAUqKDvS/HF0tERCCe6/E39gtcbYCfATPcvTLzMZqGrscvItLIPn53vwNoCVwMYGY9ww3Xd4uZjTWzBWa22MzG7+5yRERk1zWq8Ierc44DJoSmlsAfdmeFZlYA/IroCp9DgAvNbMjuLEtERHZdY8/qORs4A9gI4O4fAh13c52HA4vdfYm7bwUeA87czWXtkpqOnjj6zERE4hDHtXoaW/i3etRB7iFI+z1Y5z7ABynjy0NbGjO7wszKzKxszZo1e7C6bWq6+FX3RSTJdlr4Lfq4mWhm9wJdzOxfgReJrtQZG3e/z91L3b20Z8+eGV12HJ+gIiL5Yqdn9bi7m9l5wPXA58ABwM3uPnk317mC9Gv59wltTUZlX0SSrLGnc74JrHP3GzKwzjeA/cJZQSuAC4AvZ2C5jaYdfhFJssYW/iOAr5jZMsIBXgB3P3hXV+juleELYc8DBcAD7j5vV5ezO1xX6xERaXThPzmTK3X3vxHdtD0rDEOXbBORpGrstXqWxR1ERESaRmNP5xQRkWYiUYW/9lI9OrgrIgmWqMIvIiIJLfza4ReRJGvWhX/RqvWUjJ9UO15eWZ32LCKSRM268J9450vZjiAiknOadeEXEZH6VPhFRBJGhV9EJGFU+EVEEkaFX0QkYVT4RUQSRoVfRCRhVPhFRBKmWRf+CacMynYEEZGc06wLf/8e7WuHzzpkb24/Z5dvGCYi0uw068JfY8zg3tx1waGcf1jfnc8sItLMNevCf8SA7uzduQ3fPGFgvWn3XDQiC4lERLKvsffczUud27bktQknpLWNGzuITzeWM/bAvfjjVSM5tF9X9v2PrN3+V0SkyTXrPf6GXD16X248bQgApSXdKGhh/Omao7KcSiTzjt2/J/dcNIKbThsMwN0XHrpLr2/fqmCX1/nq+OPrtfXp2pY3bhzD0ttO2+XlNaU3bhyTNl7QovneuSNxhb8hh/bryn+fN4zzRvTJdpSc8IX9ejTYftrBRQB84/iBdGyd+T8WH/zqYfXW1ZCzDtmbM4btvcNlXTqymNk3n8Q9Fw1Pa//mCfsx/0djefH6Y3l1/PHMuGlMvddayv/3cxvYJq46dl8AhvXpzGsNFLq7vnQI0yYcz3+efeAOM2bC9Sfuv91pD33tcMYeuBdfH9Wfv3/n2LSf2YWHN3y8q1v7VgD84sJDmXfLWI4c0K3RWb55/ED26dKW3152WFr7K+OOp2fH1mltYwb3ThtP3eZ+8C9DGDUwfRu88PB+3HLmUPp1a1fb9uRVI9PmueHkAxqVc3BRpwbbUzMO2qsj7/74VKZNqP/7XXjrKZw7og9TvnNsbdshfbukzfOjs3bvd3/SkN47nykDmnVXz644Z0QfzhnRhy/s35NvPjpzh/M+dfVIzvnNtAantW9VQM+OrVn6ySYAbjptMLdOemeXstxy5lBOHNKbkf/1953OO7ioE09dPZI2hQWsWr8l7TU3nTaYVxd/zNQFaxq97gsO68tt5xzM76ctZd2mCv578kK+PWZ/vnpUCdYCurZryTWjBzL57VXMX7k+7bXTbzyBTm1aMuh7zwEw6ZujuHPyIiqqqvnnwijD2Yfuwy1nDuWSB6Yz8/11ta89/eAijhvUi0cuP4LFazZwycgSRu//AXNXfMZD05YB0Z7jU1cfRY8O0X/Qv8z+sPb1S287jbN//Soz31/HLWcO5eIjizEzxh5YxGElXXlj6VoArjthPwpaGAN7dah97f2XlHL578oAGNCjPevLK1mzvpwrjx3AhFMGc+pBe/G135YxrE9n7ruklB4dWnP5F/rTsU0hrQsLePqao5j+3qecMWxv9u7Stna5XzmimD5d23HpA9Pr/Zy/ckQ/Hn79/Ub9Toq7t2PZJ5v4x7+PZvQd/0ibduWxA1i8ekPazwLgi4fuUztsZgzoGb3f2TefxJoN5Qzs1YFbzjyQET+azOdbKmvn/em5BzO4qFPt+3j48iPrdYUe0b8br7/3KQBzfnASDnRq07J2+ugDenH5qP48N28ly9duTnvt7JtP4tNNW+nXrR0fbyhn7aatjL3rZU45sIhzhvehdWELTjmoiPMP68uQm5+vfd25I/oworgrJw/diyN+PAWAFpa+R96ugb9QJn5jFKf/4pW0tlaF9fd3Z918Ytr42eHnV9S5LXecN4yq6mrGPTWHMYN706qwBXecN4yKqm03dLrq2AFc9Yc3a8cvPrKYVxat4fl5qyi7aQylt74IwFePKuGhaUtxh9fGH89Rt237/1rzF9FfZ3/INx6dSa+OrfnG8fWPT2aCCn8dZwzbO63wH7RPZ84d0YcvHdaXLRVVVHu0V3TawUVMeusjAEYO6M5+vTvwu2nLGNi7I09ceSSLVm3gpUVruPwLA1i9vpxRA3vw1vJ13PHCwp1muGRkCRBtjIfcMrne9IG9OjCiX1ceL/uALx/el3atol9jUee2XH/i/vxscrSOy78wgMuO7s+G8kpu+vNc/lqnOIwZ3ItBe3Xi0qNKKGxhrNtcQUn3aI/q4pDhoiOL6Rr2AgFuPesgAP7nklL+NHMFFx9ZzJwVn/HZ5gp6dWwDwB+vGsmSNRsZundn7r+0FIDn5q5kc0UlZwzbh4IWxpNXjmRTRRUvzFtFl7YtGX1ATwCOGtiDo8Le3nmlfTlxSO/awv/F4X3o3alNvZ/HY1ccmTY+dO/OWEpR+Pqo/ryxdC0nD+3d4J/vY4b0Zvwpg7jt2fk8cdVIPt9cwfPzVnH16GjPvmVBVCj6dGtXu/6aDx+A4f26Mrxf13rLhai7JdUx+/fkpYVruPWsA+neoTX79mzPdY/NYnBRJzZtreTgPl3q/Z6e/9YxVFU7W+vcOa64eztaFxZw94WH8qMzD6Rjm0KeKPuA8U/Poe12umk6t2tJ53Yta9/XGzeNobLKmfTWR3z3qbfYt2eHtA+vghbGk1eNZP7K9ZwzfB/KK6rp2r4VR9/2d1as20zHlIKf6qbThzD+lEFsqqja7vp7d2pD705tmPm9E+nSrmXa76xdq0ImnDKIg/p0Ztq7nzC8X5fa17w6/nieLPsg7cMbYOS+3bnh5AM4at/uHNynCxu2VNK5XUtm3Xwit0x8m6ffXAHAjacOZvLbK1m/pZLH3viA1oUt6NKuVVhvAZu2VnHFMQNql1vzV9/YoUV0bLOtZLYsaMFendqw8vMt9O/RgfNL+/BE2fLa+X/55eFsrqiiU5uWDOvbhdkfrOMHZwxl3Njo+0VtWxXw9i0nU1HlFKZsl/8ybG+O2a8nHdsU0iKu7iZ3z/nHiBEjvCnNfH+tX/bgdJ+zfN125/l0Q7n/8u+LfPPWSq+qqvbq6mq/75/v+kfrNu9w2eUVVb5o1edePG6in/Szf/qLb6/04nET/c1ln/qPJ73tn24oT5v/t6++58XjJvq3H5/px/10qhePm+jvf7LRp85f5cXjJvq8FZ+lzb95a6X/9Ln5vrG8Iq29sqraN2+t9Ivu/z8vHjfRt1RUemVV9S7+ZLJj89ZK31Re6dXV6XnLln7ik976sHb8D/+31IvHTfRVn6f/Dp6d86EXj5voV/zuje2uo7o6+vk0pKqq2n8xZaGv3Vje4PSd2VpZ5Y++vsznrfjMK6uqfUvFtvVUV1f7L/++yD9J+b3/5Nl3vHjcRJ+7Yp0/Pv39tGU98MoSL1v6iRePm+iPTV9Wb12VVdV+1+SFvn5LRb1pO7OpvOH3v733VF5RtcvryKTyiiovHjfRb3/unZ1mX7dpq3/vz3P811MX125HFZXR63/8t7dr59taWeVbKxv/vu5/eYkXj5voazeW+8byCr9z8oIGX1+RpZ8XUOYN1FSLpuW20tJSLysry3aMrFi4aj0n3fkSj/zrERy1b8N977Jjy9duYtRPpnLvxSM4eehe2Y4j0mTMbIa7l9ZrV+EXEWmetlf4dVaPiEjCqPCLiCSMCr+ISMLkRR+/ma0Blu3my3sAH2cwTlNR7qal3E1LuZtGsbv3rNuYF4V/T5hZWUMHN3Kdcjct5W5ayp1d6uoREUkYFX4RkYRJQuG/L9sBdpNyNy3lblrKnUXNvo9fRETSJWGPX0REUqjwi4gkTLMu/GY21swWmNliMxufhfU/YGarzWxuSls3M5tsZovCc9fQbmZ2d8j6lpkNT3nNpWH+RWZ2aUr7CDObE15zt5ll5BquZtbXzKaa2dtmNs/MrsuH7GbWxsymm9nskPuHob2/mb0e1vW4mbUK7a3D+OIwvSRlWRNC+wIzOzmlPbZtyswKzGymmU3Ml9xmtjT8HmeZWVloy+ntJCy3i5n90czmm9k7ZjYyH3JnTEOX7GwOD6AAeBcYALQCZgNDmjjDMcBwYG5K2+3A+DA8HvhJGD4VeBYw4Ejg9dDeDVgSnruG4a5h2vQwr4XXnpKh3EXA8DDcEVgIDMn17GFZHcJwS+D1sI4ngAtC+z3A1WH4GuCeMHwB8HgYHhK2l9ZA/7AdFcS9TQHXA48AE8N4zucGlgI96rTl9HYSlvsQcHkYbgV0yYfcGdvWsh0gtjcGI4HnU8YnABOykKOE9MK/ACgKw0XAgjB8L3Bh3fmAC4F7U9rvDW1FwPyU9rT5MvwengFOzKfsQDvgTeAIom9aFtbdLoDngZFhuDDMZ3W3lZr54tymgD7AFOB4YGLIkQ+5l1K/8Of0dgJ0Bt4jnNySL7kz+WjOXT37AB+kjC8PbdnW290/CsMrgZqbbG4v747alzfQnlGhG+FQor3nnM8euktmAauByUR7uuvcveb+gqnrqs0Xpn8GdN+N95MJdwHfBWpus9U9T3I78IKZzTCzK0Jbrm8n/YE1wIOha+1+M2ufB7kzpjkX/pzn0e5Azp5Pa2YdgKeAb7n756nTcjW7u1e5+yFEe9CHA4OyHGmnzOx0YLW7z8h2lt0wyt2HA6cA15rZMakTc3Q7KSTqgv2Nux8KbCTq2qmVo7kzpjkX/hVA35TxPqEt21aZWRFAeLQj//wAAAO2SURBVF4d2reXd0ftfRpozwgza0lU9B9296fzKTuAu68DphJ1c3Qxs5qbpaauqzZfmN4Z+GQnuePYpo4GzjCzpcBjRN09P8+D3Lj7ivC8GvgT0Ydtrm8ny4Hl7v56GP8j0QdBrufOnGz3NcX1IPpUX0L0Z13NAa2hWchRQnof/09JP4B0exg+jfQDSNNDezei/siu4fEe0C1Mq3sA6dQMZTbgd8BdddpzOjvQE+gShtsCLwOnA0+SfpD0mjB8LekHSZ8Iw0NJP0i6hOgAaezbFDCabQd3czo30B7omDL8GjA217eTsNyXgQPC8A9C5pzPnbHtLNsBYn1z0dH4hUT9vDdmYf2PAh8BFUR7GV8n6oudAiwCXkzZUAz4Vcg6ByhNWc7XgMXhcVlKeykwN7zml9Q5WLUHuUcR/Zn7FjArPE7N9ezAwcDMkHsucHNoHxD+Iy4mKqatQ3ubML44TB+QsqwbQ7YFpJyREfc2RXrhz+ncId/s8JhXs9xc307Ccg8BysK28meiwp3zuTP10CUbREQSpjn38YuISANU+EVEEkaFX0QkYVT4RUQSRoVfRCRhVPhFdsLMbjGzMRlYzoZM5BHZUzqdU6SJmNkGd++Q7Rwi2uOXRDKziyy6dv8sM7s3XNxtg5ndadG1/KeYWc8w72/N7NwwfJtF9yl4y8zuCG0lZvb30DbFzPqF9v5mNi1cl/3WOuu/wczeCK/5YVO/f0k2FX5JHDMbDHwJONqjC7pVAV8huuxAmbsPBf4JfL/O67oDZxNd7uBgoKaY/wJ4KLQ9DNwd2n9OdCGwg4i+wV2znJOA/Yiua3MIMKLuxc1E4qTCL0l0AjACeCNcwvkEossPVAOPh3n+QHTpilSfAVuA/zWzLwKbQvtIohuoAPw+5XVHE122o6a9xknhMZPongGDiD4IRJpE4c5nEWl2jGgPfUJao9n36syXdgDM3SvN7HCiD4pzgX8jupLmjjR0EM2A/3L3e3cptUiGaI9fkmgKcK6Z9YLae8QWE/1/ODfM82XgldQXhfsTdHb3vwHfBoaFSa8RXSUToi6jl8Pwq3XaazwPfC0sDzPbpyaLSFPQHr8kjru/bWY3Ed05qgXR1VOvJbohx+Fh2mqi4wCpOgLPmFkbor3260P7N4ju5nQD0Z2dLgvt1wGPmNk4ottX1qz/hXCcYVq4B/cG4CK2Xf9dJFY6nVMk0OmWkhTq6hERSRjt8YuIJIz2+EVEEkaFX0QkYVT4RUQSRoVfRCRhVPhFRBLm/wFEFSwXEJ5FWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBo9FsQW7GXB",
        "outputId": "6c5da42a-22a9-433e-a9a9-7b8fe0215862"
      },
      "source": [
        "import rl.callbacks\r\n",
        " \r\n",
        "# ログを記録するためのクラスの定義\r\n",
        "class EpisodeLogger(rl.callbacks.Callback):\r\n",
        "    def __init__(self):\r\n",
        "        self.rewards = {}\r\n",
        "    def on_episode_begin(self, episode, logs):\r\n",
        "        self.rewards[episode] = []\r\n",
        "    def on_step_end(self, step, logs):\r\n",
        "        episode = logs['episode']\r\n",
        "        self.rewards[episode].append(logs['reward'])\r\n",
        " \r\n",
        "episode_logger = EpisodeLogger()\r\n",
        "nb_episodes = 100\r\n",
        "dqn.test(practice, nb_episodes=nb_episodes, visualize=False, callbacks=[episode_logger])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: 23.800, steps: 6\n",
            "Episode 2: reward: 24.000, steps: 5\n",
            "Episode 3: reward: 23.600, steps: 5\n",
            "Episode 4: reward: 23.800, steps: 5\n",
            "Episode 5: reward: 23.600, steps: 5\n",
            "Episode 6: reward: 23.000, steps: 7\n",
            "Episode 7: reward: 24.200, steps: 5\n",
            "Episode 8: reward: 23.800, steps: 5\n",
            "Episode 9: reward: 23.800, steps: 5\n",
            "Episode 10: reward: 24.000, steps: 5\n",
            "Episode 11: reward: 24.000, steps: 5\n",
            "Episode 12: reward: 24.200, steps: 5\n",
            "Episode 13: reward: 23.800, steps: 6\n",
            "Episode 14: reward: 23.000, steps: 7\n",
            "Episode 15: reward: 24.200, steps: 5\n",
            "Episode 16: reward: 24.000, steps: 5\n",
            "Episode 17: reward: 23.800, steps: 5\n",
            "Episode 18: reward: 23.600, steps: 5\n",
            "Episode 19: reward: 24.000, steps: 5\n",
            "Episode 20: reward: 23.000, steps: 6\n",
            "Episode 21: reward: 22.800, steps: 7\n",
            "Episode 22: reward: 23.600, steps: 5\n",
            "Episode 23: reward: 24.200, steps: 5\n",
            "Episode 24: reward: 24.200, steps: 5\n",
            "Episode 25: reward: 23.800, steps: 5\n",
            "Episode 26: reward: 23.600, steps: 6\n",
            "Episode 27: reward: 24.000, steps: 5\n",
            "Episode 28: reward: 23.400, steps: 6\n",
            "Episode 29: reward: 23.400, steps: 6\n",
            "Episode 30: reward: 21.200, steps: 9\n",
            "Episode 31: reward: 23.600, steps: 5\n",
            "Episode 32: reward: 24.200, steps: 5\n",
            "Episode 33: reward: 23.800, steps: 6\n",
            "Episode 34: reward: 23.000, steps: 6\n",
            "Episode 35: reward: 23.400, steps: 6\n",
            "Episode 36: reward: 22.400, steps: 8\n",
            "Episode 37: reward: 24.000, steps: 5\n",
            "Episode 38: reward: 24.200, steps: 5\n",
            "Episode 39: reward: 24.000, steps: 5\n",
            "Episode 40: reward: 23.800, steps: 5\n",
            "Episode 41: reward: 22.600, steps: 6\n",
            "Episode 42: reward: 23.400, steps: 6\n",
            "Episode 43: reward: 24.000, steps: 5\n",
            "Episode 44: reward: 23.800, steps: 6\n",
            "Episode 45: reward: 24.200, steps: 5\n",
            "Episode 46: reward: 23.600, steps: 5\n",
            "Episode 47: reward: 23.800, steps: 5\n",
            "Episode 48: reward: -7.400, steps: 13\n",
            "Episode 49: reward: 22.200, steps: 8\n",
            "Episode 50: reward: 23.200, steps: 7\n",
            "Episode 51: reward: 24.200, steps: 5\n",
            "Episode 52: reward: 23.800, steps: 5\n",
            "Episode 53: reward: 24.200, steps: 5\n",
            "Episode 54: reward: 23.600, steps: 5\n",
            "Episode 55: reward: 24.000, steps: 5\n",
            "Episode 56: reward: 23.000, steps: 8\n",
            "Episode 57: reward: 24.200, steps: 5\n",
            "Episode 58: reward: 23.400, steps: 6\n",
            "Episode 59: reward: 24.000, steps: 5\n",
            "Episode 60: reward: 23.200, steps: 7\n",
            "Episode 61: reward: 24.000, steps: 5\n",
            "Episode 62: reward: 23.800, steps: 6\n",
            "Episode 63: reward: 23.600, steps: 5\n",
            "Episode 64: reward: 23.800, steps: 5\n",
            "Episode 65: reward: 24.000, steps: 5\n",
            "Episode 66: reward: 22.200, steps: 8\n",
            "Episode 67: reward: 20.800, steps: 11\n",
            "Episode 68: reward: 24.000, steps: 6\n",
            "Episode 69: reward: 23.800, steps: 5\n",
            "Episode 70: reward: 24.000, steps: 6\n",
            "Episode 71: reward: 22.800, steps: 7\n",
            "Episode 72: reward: 22.600, steps: 6\n",
            "Episode 73: reward: 23.400, steps: 5\n",
            "Episode 74: reward: 22.600, steps: 8\n",
            "Episode 75: reward: 24.400, steps: 5\n",
            "Episode 76: reward: 23.800, steps: 5\n",
            "Episode 77: reward: 24.200, steps: 5\n",
            "Episode 78: reward: 24.000, steps: 5\n",
            "Episode 79: reward: 23.800, steps: 5\n",
            "Episode 80: reward: 24.000, steps: 6\n",
            "Episode 81: reward: 24.200, steps: 5\n",
            "Episode 82: reward: 23.800, steps: 5\n",
            "Episode 83: reward: 23.800, steps: 6\n",
            "Episode 84: reward: 23.200, steps: 6\n",
            "Episode 85: reward: 23.800, steps: 5\n",
            "Episode 86: reward: 24.000, steps: 5\n",
            "Episode 87: reward: 24.200, steps: 5\n",
            "Episode 88: reward: 23.600, steps: 6\n",
            "Episode 89: reward: 23.200, steps: 6\n",
            "Episode 90: reward: 24.000, steps: 5\n",
            "Episode 91: reward: 22.600, steps: 8\n",
            "Episode 92: reward: 24.200, steps: 5\n",
            "Episode 93: reward: 24.200, steps: 5\n",
            "Episode 94: reward: 24.000, steps: 5\n",
            "Episode 95: reward: 24.000, steps: 5\n",
            "Episode 96: reward: 23.600, steps: 5\n",
            "Episode 97: reward: 23.800, steps: 5\n",
            "Episode 98: reward: 23.600, steps: 5\n",
            "Episode 99: reward: 24.200, steps: 5\n",
            "Episode 100: reward: 23.800, steps: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffb6382f320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qEMcGEexHvk"
      },
      "source": [
        "import math\r\n",
        "import gym\r\n",
        "from gym import spaces, logger\r\n",
        "from gym.utils import seeding\r\n",
        "import numpy as np\r\n",
        "import copy\r\n",
        "import random\r\n",
        "\r\n",
        "class Match(Practice):\r\n",
        "    def __init__(self):\r\n",
        "        self.n_action = 5*5\r\n",
        "        self.board = [0] * self.n_action\r\n",
        "        self.action_space = gym.spaces.Discrete(self.n_action) # actionの取りうる値\r\n",
        "        self.observation_space = gym.spaces.Box(low=-1, high =1, shape=(self.n_action,)) \r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "      self.board, end_flg, reward = self.get_input(self.board, action)\r\n",
        "      done = end_flg\r\n",
        "      info = {}\r\n",
        "      return self.board, reward, done, info\r\n",
        "\r\n",
        "    def get_input(self, board, action):\r\n",
        "      end_flg = 0\r\n",
        "      reward = 0\r\n",
        "      #AI#\r\n",
        "      space = [i for i, x in enumerate(board) if x == 0 ]\r\n",
        "      if action in space:\r\n",
        "        flg = 1\r\n",
        "        board[action] = flg\r\n",
        "        end_flg, reward = self.judge(board)\r\n",
        "        if end_flg == 1:\r\n",
        "          reward = -1\r\n",
        "          print(\"Win\")\r\n",
        "        #human#\r\n",
        "        else:\r\n",
        "          print('\\n')\r\n",
        "          print(str(board[0]) + '|' + str(board[1]) + '|' + str(board[2]) + '|' + str(board[3]) + '|' + str(board[4]))\r\n",
        "          print(\"---------\")\r\n",
        "          print(str(board[5]) + '|' + str(board[6]) + '|' + str(board[7]) + '|' + str(board[8]) + '|' + str(board[9]))\r\n",
        "          print(\"---------\")\r\n",
        "          print(str(board[10]) + '|' + str(board[11]) + '|' + str(board[12]) + '|' + str(board[13]) + '|' + str(board[14]))\r\n",
        "          print(\"---------\")\r\n",
        "          print(str(board[15]) + '|' + str(board[16]) + '|' + str(board[17]) + '|' + str(board[18]) + '|' + str(board[19]))\r\n",
        "          print(\"---------\")\r\n",
        "          print(str(board[20]) + '|' + str(board[21]) + '|' + str(board[22]) + '|' + str(board[23]) + '|' + str(board[24]))\r\n",
        "          print('\\n')\r\n",
        "          space = [i for i, x in enumerate(board) if x == 0 ]\r\n",
        "          print(space)\r\n",
        "          human = input()\r\n",
        "          flg = -1\r\n",
        "          board[int(human)] = flg\r\n",
        "          end_flg, reward = self.judge(board)\r\n",
        "          if end_flg == 1:\r\n",
        "            reward = -1\r\n",
        "            print(\"Win\")\r\n",
        "      else:\r\n",
        "        end_flg = 1\r\n",
        "        reward = -1\r\n",
        "\r\n",
        "      return board, end_flg, reward\r\n",
        "   \r\n",
        "    def reset(self):\r\n",
        "        self.board = [0] * self.n_action\r\n",
        "        return self.board\r\n",
        "\r\n",
        "    def render(self, mode):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def close(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def seed(self):\r\n",
        "        pass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL7C-OFLxsp-",
        "outputId": "ccd29928-5dfc-4d77-d9ee-41c69c6bdffd"
      },
      "source": [
        "match = Match()\r\n",
        "import rl.callbacks\r\n",
        " \r\n",
        "nb_episodes = 1\r\n",
        "dqn.test(match, nb_episodes=nb_episodes, visualize=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 1 episodes ...\n",
            "\n",
            "\n",
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "\n",
            "\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "0\n",
            "\n",
            "\n",
            "-1|0|0|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "\n",
            "\n",
            "[1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "1\n",
            "\n",
            "\n",
            "-1|-1|0|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "\n",
            "\n",
            "[2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24]\n",
            "2\n",
            "\n",
            "\n",
            "-1|-1|-1|0|0\n",
            "---------\n",
            "0|0|1|1|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "\n",
            "\n",
            "[3, 4, 5, 6, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24]\n",
            "3\n",
            "\n",
            "\n",
            "-1|-1|-1|-1|0\n",
            "---------\n",
            "1|0|1|1|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|1|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "\n",
            "\n",
            "[4, 6, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24]\n",
            "4\n",
            "Win\n",
            "Episode 1: reward: -3.000, steps: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffb407570b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDDv_oi-mOxn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}